{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning from scratch: homework 2, Kaiyuan Hou 2974802"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General instructions\n",
    "\n",
    "Complete the exericse listed below in this Jupyter notebook - leaving all of your code in Python cells in the notebook itself.  Feel free to add any necessary cells.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When submitting this homework:\n",
    "\n",
    "**Make sure you have put your name at the top of each file**\n",
    "    \n",
    "**Make sure all output is present in your notebook prior to submission**\n",
    "\n",
    "**If possible please do not zip your files when uploading to canvas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Exercise 1. </span>  Perform mulclass classification on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the *multiclass softmax* cost function detailed in [Section 10.2 of the course notes](https://jermwatt.github.io/mlrefined/blog_posts/10_Linear_multiclass_classification/10_2_Multiclass_classification.html) to perform multiclass classification on a preprocessed subset of $10,000$ images from the [MNIST handwritten digit dataset](https://en.wikipedia.org/wiki/MNIST_database), which is located in the same folder as this notebook and called\n",
    "\n",
    "``mnist_contrast_normalized.csv``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you\n",
    "\n",
    "- Set the regularization parameter `lam` from the multiclass softmax to zero for your experiments\n",
    "\n",
    "\n",
    "- Use the gradient descent `Python` code block shown in [Section 6.4 of the course notes](https://jermwatt.github.io/mlrefined/blog_posts/6_First_order_methods/6_4_Gradient_descent.html). \n",
    "\n",
    "\n",
    "- You standard normalize each feature of the input to greatly speed up gradient descent - this simply involves subtracting off the mean and dividing off the standard deviation of each feature as discussed in Sections 8.4, 9.4, and 10.3 of the course notes\n",
    "\n",
    "\n",
    "- Write a custom two-panel function in `Python` to show the cost function value per iteration of gradient descent in one panel, and the corresponding number of misclassifications per iteration in the other.   You can find an efficient implementation of the multiclass misclassification counting function in Section 10.2 of the course notes \n",
    "\n",
    "\n",
    "- Use a steplength of the form $10^{\\gamma}$ where $\\gamma$ is an integer - try to find the largest steplength of this form that produces reasonable convergence.  Having normalized your input you might be surprised how large of a steplength value you can use in practice!  One way to find a working steplength is to try various values taking just a few steps (e.g., 5 or 10) of gradient descent and plotting the cost function / misclassification history plots over such short runs to visually confirm that the trend is decreasing - picking the largest steplength value that does indeed produce an overall decreasing trend, making a new run with this steplength value for a larger number of steps.\n",
    "\n",
    "\n",
    "- Using at most 300 iterations of gradient descent you should be able to learn parameters that provide less that 300 misclassifications (around 97% accuracy).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a few `Python` including one that loads in the bsaic `autograd` and `matplotlib` libraries, and one that loads in the dataset, and a suggested initialization for gradient descnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary library\n",
    "import autograd.numpy as np   \n",
    "from autograd import value_and_grad \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # this is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('mnist_test_contrast_normalized.csv',delimiter = ',')\n",
    "x = data[:,:-1].T\n",
    "y = data[:,-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use an initialization for your runs of gradient descent of the following form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = 0.1*np.random.randn(x.shape[0] + 1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def standard_normalizer(x):\n",
    "    # compute the mean and standard deviation of the input\n",
    "    x_means = np.mean(x,axis = 1)[:,np.newaxis]\n",
    "    x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n",
    "\n",
    "    # create standard normalizer function based on input data statistics\n",
    "    normalizer = lambda data: (data - x_means)/x_stds\n",
    "    \n",
    "    # return normalizer and inverse_normalizer\n",
    "    return normalizer, x_means, x_stds\n",
    "# return normalization functions based on input x\n",
    "normalizer, mean, stddev = standard_normalizer(x)\n",
    "# normalize input by subtracting off mean and dividing by standard deviation\n",
    "x = normalizer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# compute C linear combinations of input point, one per classifier\n",
    "def model(x,w):\n",
    "    # tack a 1 onto the top of each input point all at once\n",
    "    o = np.ones((1,np.shape(x)[1]))\n",
    "    x = np.vstack((o,x))\n",
    "    \n",
    "    # compute linear combination and return\n",
    "    a = np.dot(x.T,w)\n",
    "    return a\n",
    "\n",
    "# multiclass softmaax regularized by the summed length of all normal vectors\n",
    "def multiclass_softmax(w):        \n",
    "    lam = 0\n",
    "\n",
    "    # pre-compute predictions on all points\n",
    "    all_evals = model(x,w)\n",
    "    \n",
    "    # compute softmax across data points\n",
    "    a = np.log(np.sum(np.exp(all_evals),axis = 1)) \n",
    "    \n",
    "    # compute cost in compact form using numpy broadcasting\n",
    "    b = all_evals[np.arange(len(y)),y.astype(int).flatten()]\n",
    "    cost = np.sum(a - b)\n",
    "    \n",
    "    # add regularizer\n",
    "    cost = cost + lam*np.linalg.norm(w[1:,:],'fro')**2\n",
    "    \n",
    "    # return average\n",
    "    return cost/float(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient descent function - inputs: g (input function), alpha (steplength parameter), max_its (maximum number of iterations), w (initialization)\n",
    "def gradient_descent(g,alpha_choice,max_its,w):\n",
    "    # compute the gradient function of our input function - note this is a function too\n",
    "    # that - when evaluated - returns both the gradient and function evaluations (remember\n",
    "    # as discussed in Chapter 3 we always ge the function evaluation 'for free' when we use\n",
    "    # an Automatic Differntiator to evaluate the gradient)\n",
    "    gradient = value_and_grad(g)\n",
    "\n",
    "    # run the gradient descent loop\n",
    "    weight_history = []      # container for weight history\n",
    "    cost_history = []        # container for corresponding cost function history\n",
    "    alpha = 0\n",
    "    for k in range(1,max_its+1):\n",
    "        # check if diminishing steplength rule used\n",
    "        if alpha_choice == 'diminishing':\n",
    "            alpha = 1/float(k)\n",
    "        else:\n",
    "            alpha = alpha_choice\n",
    "        \n",
    "        # evaluate the gradient, store current weights and cost function value\n",
    "        cost_eval,grad_eval = gradient(w)\n",
    "        weight_history.append(w)\n",
    "        cost_history.append(cost_eval)\n",
    "\n",
    "        # take gradient descent step\n",
    "        w = w - alpha*grad_eval\n",
    "            \n",
    "    # collect final weights\n",
    "    weight_history.append(w)\n",
    "    # compute final cost function value via g itself (since we aren't computing \n",
    "    # the gradient at the final step we don't get the final cost function value \n",
    "    # via the Automatic Differentiatoor) \n",
    "    cost_history.append(g(w))  \n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\x3276\\Anaconda3\\lib\\site-packages\\autograd\\tracer.py:48: RuntimeWarning: overflow encountered in exp\n",
      "  return f_raw(*args, **kwargs)\n",
      "C:\\Users\\x3276\\Anaconda3\\lib\\site-packages\\autograd\\numpy\\numpy_vjps.py:73: RuntimeWarning: invalid value encountered in multiply\n",
      "  defvjp(anp.exp,    lambda ans, x : lambda g: ans * g)\n"
     ]
    }
   ],
   "source": [
    "# Search for best step size\n",
    "w = 0.1*np.random.randn(x.shape[0] + 1,10)\n",
    "ba = 0\n",
    "bc = 10000000000\n",
    "itr = range(101)\n",
    "alpha = list(range(2, -5, -1))\n",
    "plt.figure(1)\n",
    "for a in alpha:\n",
    "    wh, ch = gradient_descent(multiclass_softmax, 10 ** a, 100, w)\n",
    "    if (bc > ch[-1]):\n",
    "        ba = a\n",
    "        bc = ch[-1]\n",
    "    plt.plot(itr, ch)\n",
    "plt.legend([str(t) for t in alpha],loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
